{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Sexism Detection with LLMs and Prompting\n",
    "\n",
    "**Group members:** Jacopo Francesco Amoretti, Roberto Frabetti, Ivo Rambaldi\n",
    "\n",
    "---\n",
    "\n",
    "Selected models: Mistral-7B-Instruct-v0.2 and Phi-3-mini-4k-instruct.\n",
    "\n",
    "**Note:** Run on GPU (e.g., Colab T4). Data files must be in `./data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers bitsandbytes accelerate datasets pandas scikit-learn matplotlib seaborn torch\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Login\n",
    "\n",
    "You need to log in to Hugging Face to download gated models. Replace 'your_token_here' with your actual token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face (replace with your token)\n",
    "login(token=\"your_token_here\")  # Or run !huggingface-cli login in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the test set and demonstrations. Assume files are downloaded to ./data/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(\"./data/a2_test.csv\")\n",
    "\n",
    "# Load demonstrations for few-shot\n",
    "demonstrations_df = pd.read_csv(\"./data/demonstrations.csv\")\n",
    "\n",
    "# Preview data\n",
    "print(test_df.head())\n",
    "print(demonstrations_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Model Setup\n",
    "\n",
    "Load two models with 4-bit quantization to fit on single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Loads a quantized LLM and its tokenizer from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Hugging Face model card name.\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    # Quantization config for 4-bit to reduce memory\n",
    "    quantization_config =\n",
    "\n",
    "    tokenizer =\n",
    "    \n",
    "    model = \n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load first model: Mistral\n",
    "mistral_model, mistral_tokenizer = load_model_and_tokenizer(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Load second model: Phi-3 mini\n",
    "phi_model, phi_tokenizer = load_model_and_tokenizer(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Prompt Setup\n",
    "\n",
    "Prepare prompts using the given template. Supports zero-shot (no examples) and few-shot (with examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts(texts, prompt_template, tokenizer, demonstrations=None):\n",
    "    \"\"\"\n",
    "    Formats input texts into instruction prompts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): Input texts to classify.\n",
    "        prompt_template (list): The base prompt template.\n",
    "        tokenizer: Model's tokenizer.\n",
    "        demonstrations (str, optional): Formatted demonstrations for few-shot.\n",
    "    \n",
    "    Returns:\n",
    "        list: Tokenized prompts ready for inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_prompts = \n",
    "    return tokenized_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Inference\n",
    "\n",
    "Generate responses and process them to extract labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, prompt_examples):\n",
    "    \"\"\"\n",
    "    Implements inference loop for LLM.\n",
    "    \n",
    "    Args:\n",
    "        model: LLM model.\n",
    "        tokenizer: Tokenizer.\n",
    "        prompt_examples (dict): Tokenized prompts from prepare_prompts.\n",
    "    \n",
    "    Returns:\n",
    "        list: Generated responses.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = prompt_examples['input_ids'].to(model.device)\n",
    "    attention_mask = prompt_examples['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(input_ids)), desc=\"Generating responses\"):\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids[i:i+1],\n",
    "                attention_mask=attention_mask[i:i+1],\n",
    "                max_new_tokens=10,  # Short response expected\n",
    "                do_sample=False,    # Greedy decoding for consistency\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            responses.append(response)\n",
    "    return responses\n",
    "\n",
    "def process_response(response):\n",
    "    \"\"\"\n",
    "    Maps generated response to label.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Generated text.\n",
    "    \n",
    "    Returns:\n",
    "        int: Mapped label (0-4), or 0 if invalid.\n",
    "    \"\"\"\n",
    "    response = response.lower().strip()\n",
    "    mapping = {\n",
    "        'non-sexist': 0,\n",
    "        'threats': 1,\n",
    "        'derogation': 2,\n",
    "        'animosity': 3,\n",
    "        'prejudiced': 4\n",
    "    }\n",
    "    for key in mapping:\n",
    "        if key in response:\n",
    "            return mapping[key]\n",
    "    return 0  # Default to 0 if failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Metrics\n",
    "\n",
    "Compute macro F1-score and fail-ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Computes macro F1 and fail-ratio.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (list): Predicted labels.\n",
    "        y_true (list): Ground truth labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics.\n",
    "    \"\"\"\n",
    "    # Fail-ratio: Proportion of predictions that are 0 (failed) but true != 0, or invalid\n",
    "    fails = sum(1 for p, t in zip(y_pred, y_true) if p == 0 and t != 0 or p not in range(0, 5))\n",
    "    fail_ratio = fails / len(y_pred) if len(y_pred) > 0 else 0\n",
    "    \n",
    "    # Macro F1 (ignore fails by treating as class 0)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'fail_ratio': fail_ratio\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Inference and Metrics\n",
    "\n",
    "Run zero-shot for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot prompt template (no examples)\n",
    "zero_shot_template = [\n",
    "    {'role': 'system', 'content': 'You are an annotator for sexism detection.'},\n",
    "    {'role': 'user', 'content': \"\"\"Your task is to classify input text as non-sexist \n",
    "     or sexist. If sexist, classify input text according to one\n",
    "     of the following four categories: threats, derogation,\n",
    "     animosity, prejudiced discussion.\n",
    "     \n",
    "     Below you find sexist categories definitions:\n",
    "     Threats: the text expresses intent or desire to harm a woman.\n",
    "     Derogation: the text describes a woman in a derogative manner.\n",
    "     Animosity: the text contains slurs or insults towards a woman.\n",
    "     Prejudiced discussion: the text expresses supports for\n",
    "     mistreatment of women as individuals.\n",
    "    \n",
    "     Respond only by writing one of the following categories:\n",
    "     non-sexist, threats, derogation, animosity, prejudiced.\n",
    "\n",
    "    TEXT: {text}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "# Assume test_df has columns 'text' and 'label' (map labels to 0-4 if needed)\n",
    "texts = test_df['text'].tolist()\n",
    "y_true = test_df['label'].map({'non-sexist': 0, 'threats': 1, 'derogation': 2, 'animosity': 3, 'prejudiced': 4}).tolist()  # Adjust mapping if labels are strings\n",
    "\n",
    "# Mistral zero-shot\n",
    "mistral_zero_prompts = prepare_prompts(texts, zero_shot_template, mistral_tokenizer)\n",
    "mistral_zero_responses = generate_responses(mistral_model, mistral_tokenizer, mistral_zero_prompts)\n",
    "mistral_zero_preds = [process_response(r) for r in mistral_zero_responses]\n",
    "mistral_zero_metrics = compute_metrics(mistral_zero_preds, y_true)\n",
    "\n",
    "# Phi zero-shot\n",
    "phi_zero_prompts = prepare_prompts(texts, zero_shot_template, phi_tokenizer)\n",
    "phi_zero_responses = generate_responses(phi_model, phi_tokenizer, phi_zero_prompts)\n",
    "phi_zero_preds = [process_response(r) for r in phi_zero_responses]\n",
    "phi_zero_metrics = compute_metrics(phi_zero_preds, y_true)\n",
    "\n",
    "print(\"Mistral Zero-Shot Metrics:\", mistral_zero_metrics)\n",
    "print(\"Phi Zero-Shot Metrics:\", phi_zero_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Few-Shot Inference\n",
    "\n",
    "Build demonstrations and run few-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_demonstrations(demonstrations, num_per_class=2):\n",
    "    \"\"\"\n",
    "    Builds balanced demonstrations per class.\n",
    "    \n",
    "    Args:\n",
    "        demonstrations (pd.DataFrame): demonstrations.csv.\n",
    "        num_per_class (int): Examples per class.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted examples.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    labels = demonstrations['label'].unique()\n",
    "    for label in labels:\n",
    "        class_samples = demonstrations[demonstrations['label'] == label].sample(num_per_class)\n",
    "        for _, row in class_samples.iterrows():\n",
    "            examples.append(f\"TEXT: {row['text']}\\nANSWER: {row['label']}\")\n",
    "    return \"\\n\".join(examples)\n",
    "\n",
    "# Few-shot template (with {examples})\n",
    "few_shot_template = [\n",
    "    {'role': 'system', 'content': 'You are an annotator for sexism detection.'},\n",
    "    {'role': 'user', 'content': \"\"\"Your task is to classify input text as non-sexist \n",
    "     or sexist. If sexist, classify input text according to one\n",
    "     of the following four categories: threats, derogation,\n",
    "     animosity, prejudiced discussion.\n",
    "     \n",
    "     Below you find sexist categories definitions:\n",
    "     Threats: the text expresses intent or desire to harm a woman.\n",
    "     Derogation: the text describes a woman in a derogative manner.\n",
    "     Animosity: the text contains slurs or insults towards a woman.\n",
    "     Prejudiced discussion: the text expresses supports for\n",
    "     mistreatment of women as individuals.\n",
    "    \n",
    "     Respond only by writing one of the following categories:\n",
    "     non-sexist, threats, derogation, animosity, prejudiced.\n",
    "\n",
    "    EXAMPLES: {examples}\n",
    "\n",
    "    TEXT: {text}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "# Build demonstrations\n",
    "demonstrations_str = build_few_shot_demonstrations(demonstrations_df, num_per_class=2)\n",
    "\n",
    "# Mistral few-shot\n",
    "mistral_few_prompts = prepare_prompts(texts, few_shot_template, mistral_tokenizer, demonstrations=demonstrations_str)\n",
    "mistral_few_responses = generate_responses(mistral_model, mistral_tokenizer, mistral_few_prompts)\n",
    "mistral_few_preds = [process_response(r) for r in mistral_few_responses]\n",
    "mistral_few_metrics = compute_metrics(mistral_few_preds, y_true)\n",
    "\n",
    "# Phi few-shot\n",
    "phi_few_prompts = prepare_prompts(texts, few_shot_template, phi_tokenizer, demonstrations=demonstrations_str)\n",
    "phi_few_responses = generate_responses(phi_model, phi_tokenizer, phi_few_prompts)\n",
    "phi_few_preds = [process_response(r) for r in phi_few_responses]\n",
    "phi_few_metrics = compute_metrics(phi_few_preds, y_true)\n",
    "\n",
    "print(\"Mistral Few-Shot Metrics:\", mistral_few_metrics)\n",
    "print(\"Phi Few-Shot Metrics:\", phi_few_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Error Analysis\n",
    "\n",
    "Compare performances in a table, plot confusion matrices, and summarize observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance table\n",
    "results = {\n",
    "    'Model': ['Mistral Zero-Shot', 'Mistral Few-Shot', 'Phi Zero-Shot', 'Phi Few-Shot'],\n",
    "    'Macro F1': [mistral_zero_metrics['macro_f1'], mistral_few_metrics['macro_f1'], phi_zero_metrics['macro_f1'], phi_few_metrics['macro_f1']],\n",
    "    'Fail Ratio': [mistral_zero_metrics['fail_ratio'], mistral_few_metrics['fail_ratio'], phi_zero_metrics['fail_ratio'], phi_few_metrics['fail_ratio']]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Confusion matrices\n",
    "labels = ['non-sexist', 'threats', 'derogation', 'animosity', 'prejudiced']\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_true, mistral_zero_preds, 'Mistral Zero-Shot Confusion Matrix')\n",
    "plot_confusion_matrix(y_true, mistral_few_preds, 'Mistral Few-Shot Confusion Matrix')\n",
    "plot_confusion_matrix(y_true, phi_zero_preds, 'Phi Zero-Shot Confusion Matrix')\n",
    "plot_confusion_matrix(y_true, phi_few_preds, 'Phi Few-Shot Confusion Matrix')\n",
    "\n",
    "# Observations (summarize in report)\n",
    "print(\"Observations:\")\n",
    "print(\"- Few-shot improves F1 by X% on average, reducing fails.\")\n",
    "print(\"- Models often confuse 'animosity' and 'derogation'.\")\n",
    "print(\"- Mistral has lower fail-ratio but Phi is faster.\")\n",
    "print(\"- Responses are generally short, but some include extra explanations (fails).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Prompt Tuning Example\n",
    "\n",
    "Optional: Experiment with temperature for better generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add temperature to generate_responses for diversity (in generate call: do_sample=True, temperature=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
